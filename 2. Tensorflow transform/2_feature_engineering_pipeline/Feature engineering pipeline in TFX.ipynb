{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering Pipeline"
      ],
      "metadata": {
        "id": "rJJhbCrj4CX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In production-grade projects, you want to streamline tasks so you can more easily improve your model or find issues that may arise. [Tensorflow Extended (TFX)](https://www.tensorflow.org/tfx) provides components that work together to execute the most common steps in a machine learning project.\n",
        "\n",
        "In this notebook, I'll take you through the steps to build your own feature engineering pipeline. Specifically, I will:\n",
        "\n",
        "* ingest data from a base directory with `ExampleGen`\n",
        "* compute the statistics of the training data with `StatisticsGen`\n",
        "* infer a schema with `SchemaGen`\n",
        "* detect anomalies in the evaluation data with `ExampleValidator`\n",
        "* preprocess the data into features suitable for model training with `Transform`\n",
        "\n",
        "The components I will use are the orange boxes highlighted in the figure below:\n",
        "\n",
        "<img src='img/feature_eng_pipeline.png'>"
      ],
      "metadata": {
        "id": "J9h_bxs94ONW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "u0wnS_ip4729"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_qsoIGJbxBnl"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tfx\n",
        "\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define paths"
      ],
      "metadata": {
        "id": "Qs6sNAUE5RaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# location of the pipeline metadata store\n",
        "_pipeline_root = './pipeline/'\n",
        "\n",
        "# directory of the raw data files\n",
        "_data_root = './data/census_data'\n",
        "\n",
        "# path to the raw training data\n",
        "_data_filepath = os.path.join(_data_root, 'adult.data')"
      ],
      "metadata": {
        "id": "Upr3pZLl5KRk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preview the dataset\n",
        "\n",
        "I will be using the [Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult). The data can be used to predict if an individual earns more than or less than 50k US Dollars annually. Here is the description of the features:\n",
        "\n",
        "\n",
        "* **age**: continuous.\n",
        "* **workclass**: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
        "* **fnlwgt**: continuous.\n",
        "* **education**: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
        "* **education-num**: continuous.\n",
        "* **marital-status**: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
        "* **occupation**: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
        "* **relationship**: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
        "* **race**: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
        "* **sex**: Female, Male.\n",
        "* **capital-gain**: continuous.\n",
        "* **capital-loss**: continuous.\n",
        "* **hours-per-week**: continuous.\n",
        "* **native-country**: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands."
      ],
      "metadata": {
        "id": "TzLg7knC6g-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interactive context\n",
        "\n",
        "When pushing to production, you want to automate the pipeline execution using orchestrators such as [Apache Beam](https://beam.apache.org/) and [Kubeflow](https://www.kubeflow.org/). I will not be doing that just yet and will instead execute the pipeline from this notebook. When experimenting in a notebook environment, I will be *manually* executing the pipeline components (i.e. I am the orchestrator). For that, TFX provides the [Interactive Context](https://github.com/tensorflow/tfx/blob/master/tfx/orchestration/experimental/interactive/interactive_context.py)."
      ],
      "metadata": {
        "id": "YtLneOo163mA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will initialize the `InteractiveContext` below. This will create a database in the `_pipeline_root` directory which the different components will use to save or get the state of the component executions.\n",
        "\n",
        "*Note: You can configure the database to connect to but for this exercise, I will just use the default which is a newly created local sqlite file.*"
      ],
      "metadata": {
        "id": "HxSFuk787GCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = InteractiveContext(pipeline_root=_pipeline_root)"
      ],
      "metadata": {
        "id": "nFJ4ETlU6_2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run TFX components interactively\n",
        "\n",
        "With that, you can now run the pipeline interactively. You will see how to do that as you go through the different components below."
      ],
      "metadata": {
        "id": "1CqFYRqB7lqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ExampleGen\n",
        "\n",
        "I will start the pipeline with the [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) component. This  will:\n",
        "\n",
        "*   split the data into training and evaluation sets (by default: 2/3 train, 1/3 eval).\n",
        "*   convert each data row into `tf.train.Example` format. This [protocol buffer](https://developers.google.com/protocol-buffers) is designed for Tensorflow operations and is used by the TFX components.\n",
        "*   compress and save the data collection under the `_pipeline_root` directory for other components to access. These examples are stored in `TFRecord` format. This optimizes read and write operations within Tensorflow especially if you have a large collection of data.\n"
      ],
      "metadata": {
        "id": "S3jls4Yg7qeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate ExampleGen with the input CSV dataset\n",
        "example_gen = tfx.components.CsvExampleGen(input_base=_data_root)"
      ],
      "metadata": {
        "id": "0hfXRhug7moQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the component\n",
        "context.run(example_gen)"
      ],
      "metadata": {
        "id": "azfMuS657-SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the artifact object\n",
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "\n",
        "# print split names and uri\n",
        "print(f'split names: {artifact.split_names}')\n",
        "print(f'artifact uri: {artifact.uri}')"
      ],
      "metadata": {
        "id": "8QZWglgV8EGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the URI of the output artifact representing the training examples\n",
        "train_uri = os.path.join(artifact.uri, 'Split-train')\n",
        "\n",
        "# See the contents of the `train` folder\n",
        "!ls {train_uri}"
      ],
      "metadata": {
        "id": "Sa0GSGpk8DyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a notebook environment, it may be useful to examine a few examples of the data especially if you're still experimenting. Since the data collection is saved in [TFRecord format](https://www.tensorflow.org/tutorials/load_data/tfrecord), you will need to use methods that work with that data type. You will need to unpack the individual examples from the `TFRecord` file and format it for printing. Let's do that in the following cells:"
      ],
      "metadata": {
        "id": "xH8cBNCJ8T9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ],
      "metadata": {
        "id": "1WISW3Ut8UJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a helper function to get individual examples\n",
        "def get_records(dataset, num_records):\n",
        "    '''Extracts records from the given dataset.\n",
        "    Args:\n",
        "        dataset (TFRecordDataset): dataset saved by ExampleGen\n",
        "        num_records (int): number of records to preview\n",
        "    '''\n",
        "\n",
        "    # initialize an empty list\n",
        "    records = []\n",
        "\n",
        "    # Use the `take()` method to specify how many records to get\n",
        "    for tfrecord in dataset.take(num_records):\n",
        "\n",
        "        # Get the numpy property of the tensor\n",
        "        serialized_example = tfrecord.numpy()\n",
        "\n",
        "        # Initialize a `tf.train.Example()` to read the serialized data\n",
        "        example = tf.train.Example()\n",
        "\n",
        "        # Read the example data (output is a protocol buffer message)\n",
        "        example.ParseFromString(serialized_example)\n",
        "\n",
        "        # convert the protocol bufffer message to a Python dictionary\n",
        "        example_dict = (MessageToDict(example))\n",
        "\n",
        "        # append to the records list\n",
        "        records.append(example_dict)\n",
        "\n",
        "    return records"
      ],
      "metadata": {
        "id": "CC6V0y9T8fk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records = get_records(dataset, 3)\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records)"
      ],
      "metadata": {
        "id": "UK-WceI58oDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that `ExampleGen` has finished ingesting the data, the next step is data analysis."
      ],
      "metadata": {
        "id": "JS6HGY758qk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StatisticsGen\n",
        "The [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) component computes statistics over your dataset for data analysis, as well as for use in downstream components (i.e. next steps in the pipeline).\n",
        "\n",
        "`StatisticsGen` takes as input the dataset we just ingested using `CsvExampleGen`."
      ],
      "metadata": {
        "id": "XCQGYF3897iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate StatisticsGen with the ExampleGen ingested dataset\n",
        "statistics_gen = tfx.components.StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "# Execute the component\n",
        "context.run(statistics_gen)"
      ],
      "metadata": {
        "id": "UpyGG2vc8qv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the output statistics\n",
        "context.show(statistics_gen.outputs['statistics'])"
      ],
      "metadata": {
        "id": "IqcXS1bHS5Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SchemaGen\n",
        "\n",
        "The [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component also uses TFDV to generate a schema based on your data statistics.\n",
        "\n",
        "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."
      ],
      "metadata": {
        "id": "bgW38UMzTB1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate SchemaGen with the StatisticsGen ingested dataset\n",
        "schema_gen = tfx.components.SchemaGen(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    )\n",
        "\n",
        "# Run the component\n",
        "context.run(schema_gen)"
      ],
      "metadata": {
        "id": "LnKrRSTzTGAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the schema\n",
        "context.show(schema_gen.outputs['schema'])"
      ],
      "metadata": {
        "id": "48xuHbYfTlGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now move to the next step in the pipeline and see if there are any anomalies in the data."
      ],
      "metadata": {
        "id": "hgzXk1vBTnl2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hvfzxfIcTnvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ExampleValidator\n",
        "\n",
        "The [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component detects anomalies in your data based on the generated schema from the previous step.\n",
        "\n",
        "`ExampleValidator` will take as input the statistics from `StatisticsGen` and the schema from `SchemaGen`. By default, it compares the statistics from the evaluation split to the schema from the training split."
      ],
      "metadata": {
        "id": "TSQgw1NQTo1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n",
        "example_validator = tfx.components.ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "\n",
        "# Run the component.\n",
        "context.run(example_validator)"
      ],
      "metadata": {
        "id": "RrI-bvReTvZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\n",
        "context.show(example_validator.outputs['anomalies'])"
      ],
      "metadata": {
        "id": "ALpsnHfFT4gE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform\n",
        "The [Transform](https://www.tensorflow.org/tfx/guide/transform) component performs feature engineering for both training and serving datasets. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library.\n",
        "\n",
        "`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module containing the preprocessing function.\n",
        "\n",
        "Now, I will code an example of a user-defined Transform code. The pipeline needs to load this as a module so we need to use the magic command `%% writefile` to save the file to disk."
      ],
      "metadata": {
        "id": "Stesx3IWT8k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the constants module filename\n",
        "_census_constants_module_file = 'census_constants.py'"
      ],
      "metadata": {
        "id": "cQoNl-3eT4pn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {_census_constants_module_file}\n",
        "\n",
        "# Features with string data types that will be converted to indices\n",
        "CATEGORICAL_FEATURE_KEYS = [\n",
        "    'education', 'marital-status', 'occupation', 'race', 'relationship', 'workclass', 'sex', 'native-country'\n",
        "]\n",
        "\n",
        "# Numerical features that are marked as continuous\n",
        "NUMERIC_FEATURE_KEYS = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "# Feature that can be grouped into buckets\n",
        "BUCKET_FEATURE_KEYS = ['age']\n",
        "\n",
        "# Number of buckets used by tf.transform for encoding each bucket feature.\n",
        "FEATURE_BUCKET_COUNT = {'age': 4}\n",
        "\n",
        "# Feature that the model will predict\n",
        "LABEL_KEY = 'label'\n",
        "\n",
        "# Utility function for renaming the feature\n",
        "def transformed_name(key):\n",
        "    return key + '_xf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbwYEd_sUQ29",
        "outputId": "4b039058-d4a8-4fc3-b973-cb03498b8ae9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing census_constants.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the transform module filename\n",
        "_census_transform_module_file = 'census_transform.py'"
      ],
      "metadata": {
        "id": "GDahQp_SUd3g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {_census_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "import census_constants\n",
        "\n",
        "# Unpack the contents of the constants module\n",
        "_NUMERIC_FEATURE_KEYS = census_constants.NUMERIC_FEATURE_KEYS\n",
        "_CATEGORICAL_FEATURE_KEYS = census_constants.CATEGORICAL_FEATURE_KEYS\n",
        "_BUCKET_FEATURE_KEYS = census_constants.BUCKET_FEATURE_KEYS\n",
        "_FEATURE_BUCKET_COUNT = census_constants.FEATURE_BUCKET_COUNT\n",
        "_LABEL_KEY = census_constants.LABEL_KEY\n",
        "_transformed_name = census_constants.transformed_name\n",
        "\n",
        "\n",
        "# Define the transformations\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "    Args:\n",
        "        inputs: map from feature keys to raw not-yet-transformed features.\n",
        "    Returns:\n",
        "        Map from string feature key to transformed feature operations.\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    # Scale these features to the range [0,1]\n",
        "    for key in _NUMERIC_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.scale_to_0_1(\n",
        "            inputs[key])\n",
        "\n",
        "    # Bucketize these features\n",
        "    for key in _BUCKET_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.bucketize(\n",
        "            inputs[key], _FEATURE_BUCKET_COUNT[key])\n",
        "\n",
        "    # Convert strings to indices in a vocabulary\n",
        "    for key in _CATEGORICAL_FEATURE_KEYS:\n",
        "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(inputs[key])\n",
        "\n",
        "    # Convert the label strings to an index\n",
        "    outputs[_transformed_name(_LABEL_KEY)] = tft.compute_and_apply_vocabulary(inputs[_LABEL_KEY])\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGrEYrNwUXDX",
        "outputId": "ce6d1d1c-7fd7-4e1b-8090-df9d137a20e1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing census_transform.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now pass the training data, schema, and transform module to the `Transform` component. You can ignore the warning messages generated by Apache Beam regarding type hints."
      ],
      "metadata": {
        "id": "f8U873jEU11B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore TF warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Instantiate the Transform component\n",
        "transform = tfx.components.Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_census_transform_module_file))\n",
        "\n",
        "# Run the component\n",
        "context.run(transform)"
      ],
      "metadata": {
        "id": "aFk6laZDUwsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the output artifacts of `Transform` (i.e. `.component.outputs` from the output cell above). This component produces several outputs:\n",
        "\n",
        "* `transform_graph` is the graph that can perform the preprocessing operations. This graph will be included during training and serving to ensure consistent transformations of incoming data.\n",
        "* `transformed_examples` points to the preprocessed training and evaluation data.\n",
        "* `updated_analyzer_cache` are stored calculations from previous runs."
      ],
      "metadata": {
        "id": "-lAPrTn_VJMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a peek at the `transform_graph` artifact.  It points to a directory containing three subdirectories."
      ],
      "metadata": {
        "id": "0Np9S6ySVNSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the uri of the transform graph\n",
        "transform_graph_uri = transform.outputs['transform_graph'].get()[0].uri\n",
        "\n",
        "# List the subdirectories under the uri\n",
        "os.listdir(transform_graph_uri)"
      ],
      "metadata": {
        "id": "KMB32IpJVLDw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `metadata` subdirectory contains the schema of the original data.\n",
        "* The `transformed_metadata` subdirectory contains the schema of the preprocessed data.\n",
        "* The `transform_fn` subdirectory contains the actual preprocessing graph.\n",
        "\n",
        "You can also take a look at the first three transformed examples using the helper function defined earlier."
      ],
      "metadata": {
        "id": "sjX6sq80VWp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the URI of the output artifact representing the transformed examples\n",
        "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
        "\n",
        "# Get the list of files in this directory (all compressed TFRecord files)\n",
        "tfrecord_filenames = [os.path.join(train_uri, name)\n",
        "                      for name in os.listdir(train_uri)]\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"
      ],
      "metadata": {
        "id": "1eXC3Q43VPio"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 3 records from the dataset\n",
        "sample_records_xf = get_records(transformed_dataset, 3)\n",
        "\n",
        "# Print the output\n",
        "pp.pprint(sample_records_xf)"
      ],
      "metadata": {
        "id": "PaxQ_Q4kVe7z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}