{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering using tensorflow transform"
      ],
      "metadata": {
        "id": "552fbeY3pmuB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2_qTVDBCkrKr",
        "outputId": "def7478f-b018-47db-b9b7-52631915cf86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_transform\n",
            "  Downloading tensorflow_transform-1.14.0-py3-none-any.whl (447 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py<2.0.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from tensorflow_transform) (1.4.0)\n",
            "Collecting apache-beam[gcp]<3,>=2.47 (from tensorflow_transform)\n",
            "  Downloading apache_beam-2.53.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_transform) (1.25.2)\n",
            "Requirement already satisfied: protobuf<5,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_transform) (3.20.3)\n",
            "Requirement already satisfied: pyarrow<11,>=10 in /usr/local/lib/python3.10/dist-packages (from tensorflow_transform) (10.0.1)\n",
            "Requirement already satisfied: pydot<2,>=1.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_transform) (1.4.2)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow_transform) (2.15.0)\n",
            "Requirement already satisfied: tensorflow-metadata<1.15.0,>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_transform) (1.14.0)\n",
            "Collecting tfx-bsl<1.15.0,>=1.14.0 (from tensorflow_transform)\n",
            "  Downloading tfx_bsl-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting crcmod<2.0,>=1.7 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4,>=3.9.7 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading orjson-3.9.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading fastavro-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (1.60.1)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.22.0)\n",
            "Collecting js2py<1,>=0.74 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (4.19.2)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (3.0.2)\n",
            "Collecting numpy>=1.22.0 (from tensorflow_transform)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting objsize<0.7.0,>=0.6.1 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (23.2)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (1.23.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2023.4)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2023.12.25)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (4.9.0)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix<1 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.6)\n",
            "Requirement already satisfied: cachetools<6,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (5.3.2)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.11.1)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.5/173.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.1.1)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.15.2)\n",
            "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_pubsub-2.19.4-py2.py3-none-any.whl (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.2/273.2 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_pubsublite-1.9.0-py2.py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-storage<3,>=2.10.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_storage-2.14.0-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (3.12.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.24.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.3.3)\n",
            "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_bigtable-2.23.0-py2.py3-none-any.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.5/357.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_spanner-3.42.0-py2.py3-none-any.whl (355 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.5/355.5 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_dlp-3.15.1-py2.py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.8/159.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.9.1)\n",
            "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_videointelligence-2.13.1-py2.py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.5/240.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_vision-3.7.0-py2.py3-none-any.whl (460 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.0/460.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_cloud_recommendations_ai-0.10.8-py2.py3-none-any.whl (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.4/180.4 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam[gcp]<3,>=2.47->tensorflow_transform) (1.39.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot<2,>=1.2->tensorflow_transform) (3.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (0.36.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13->tensorflow_transform) (2.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata<1.15.0,>=1.14.0->tensorflow_transform) (1.62.0)\n",
            "Collecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.15.0,>=1.14.0->tensorflow_transform)\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tfx-bsl<1.15.0,>=1.14.0->tensorflow_transform) (1.5.3)\n",
            "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.15.0,>=1.14.0->tensorflow_transform)\n",
            "  Downloading tensorflow_serving_api-2.14.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.13->tensorflow_transform) (0.42.0)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.15.0,>=1.14.0->tensorflow_transform)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (1.12.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.0.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.7.0)\n",
            "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading google_api_core-2.17.0-py3-none-any.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.9/136.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.13.0)\n",
            "Requirement already satisfied: grpcio-status>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (1.48.2)\n",
            "Collecting overrides<8.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.4.4)\n",
            "Collecting deprecated>=1.2.14 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3,>=2.10.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (1.5.0)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.10/dist-packages (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (5.2)\n",
            "Collecting pyjsparser>=2.5.1 (from js2py<1,>=0.74->apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.17.1)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform)\n",
            "  Downloading dnspython-2.5.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.4/305.4 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (2024.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_transform) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_transform) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_transform) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_transform) (3.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_transform) (1.3.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47->tensorflow_transform) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_transform) (2.1.5)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13->tensorflow_transform) (3.2.2)\n",
            "Building wheels for collected packages: crcmod, dill, google-apitools, hdfs, pyjsparser, docopt\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31409 sha256=8eb930281b79871f67d27619d5f63d14a51397d6ee41d3e6e7b7e82a02a72b35\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78540 sha256=3f761896a0d0343c1f206ee9a107447f270f2460f7430a779873fd62cdec8696\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for google-apitools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131015 sha256=2ca48b2a946bf1ba019419299fa4e44cecb3f3f50b8c9678f82a84c1e49e9e2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34325 sha256=7dc6f5b66e0c28b0373cc8b96d0d47fb3dfd9e8faaa6c1ba171c96886701af1f\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
            "  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=25984 sha256=cc70a06bb0340d11c707a9f5ab9088a8caee8fff4e890180cc8637efbbd1deb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=4f30642e618c37593f2536ceaaca86bcb50a87066b59c1b59304c0e9c850eba2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built crcmod dill google-apitools hdfs pyjsparser docopt\n",
            "Installing collected packages: pyjsparser, docopt, crcmod, zstandard, uritemplate, overrides, orjson, objsize, numpy, js2py, grpc-interceptor, fasteners, fastavro, dnspython, dill, deprecated, pymongo, hdfs, google-apitools, google-api-core, google-api-python-client, apache-beam, google-cloud-vision, google-cloud-videointelligence, google-cloud-storage, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-dlp, google-cloud-bigtable, tensorflow-serving-api, google-cloud-pubsublite, tfx-bsl, tensorflow_transform\n",
            "  Attempting uninstall: uritemplate\n",
            "    Found existing installation: uritemplate 4.1.1\n",
            "    Uninstalling uritemplate-4.1.1:\n",
            "      Successfully uninstalled uritemplate-4.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: google-api-core\n",
            "    Found existing installation: google-api-core 2.11.1\n",
            "    Uninstalling google-api-core-2.11.1:\n",
            "      Successfully uninstalled google-api-core-2.11.1\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.53.0 crcmod-1.7 deprecated-1.2.14 dill-0.3.1.1 dnspython-2.5.0 docopt-0.6.2 fastavro-1.9.3 fasteners-0.19 google-api-core-2.17.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-bigtable-2.23.0 google-cloud-dlp-3.15.1 google-cloud-pubsub-2.19.4 google-cloud-pubsublite-1.9.0 google-cloud-recommendations-ai-0.10.8 google-cloud-spanner-3.42.0 google-cloud-storage-2.14.0 google-cloud-videointelligence-2.13.1 google-cloud-vision-3.7.0 grpc-interceptor-0.15.4 hdfs-2.7.3 js2py-0.74 numpy-1.24.4 objsize-0.6.1 orjson-3.9.13 overrides-7.7.0 pyjsparser-2.7.1 pymongo-4.6.1 tensorflow-serving-api-2.14.1 tensorflow_transform-1.14.0 tfx-bsl-1.14.0 uritemplate-3.0.1 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "googleapiclient",
                  "numpy",
                  "uritemplate"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n",
            "TFX Transform version: 1.14.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "!pip install tensorflow_transform\n",
        "import tensorflow_transform as tft\n",
        "import tensorflow_transform.beam as tft_beam\n",
        "\n",
        "from tensorflow_transform.tf_metadata import dataset_metadata\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "print(f'TensorFlow version: {tf.__version__}')\n",
        "print(f'TFX Transform version: {tft.__version__}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect raw data"
      ],
      "metadata": {
        "id": "5nigCEQ-loht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define sample data\n",
        "raw_data = [\n",
        "      {'x': 1, 'y': 1, 's': 'hello'},\n",
        "      {'x': 2, 'y': 2, 's': 'world'},\n",
        "      {'x': 3, 'y': 3, 's': 'hello'}\n",
        "  ]"
      ],
      "metadata": {
        "id": "hbCR8aiRlmcf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the metadata"
      ],
      "metadata": {
        "id": "qWGIGdBXlu9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The transform function later expects the metadata to be packed in a [DatasetMetadata](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/dataset_metadata.py#L23) object.\n",
        "* The constructor for the `DatasetMetadata` class expects a [Schema protocol buffer](https://github.com/tensorflow/metadata/blob/master/tensorflow_metadata/proto/v0/schema.proto#L46) data type. You can use the [schema_from_feature_spec()](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/tf_metadata/schema_utils.py#L36) method to generate that from a dictionary.\n",
        "* To build the said dictionary, you will use the keys/column names of `raw_data` and assign a [FeatureSpecType](https://github.com/tensorflow/transform/blob/master/tensorflow_transform/common_types.py#L29) as values. This allows you to specify if the input is fixed or variable length (using [tf.io](https://www.tensorflow.org/api_docs/python/tf/io) classes), as well as to define the shape and data type."
      ],
      "metadata": {
        "id": "pGEbbFUmmVSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the schema as a DatasetMetadata object\n",
        "raw_data_metadata = dataset_metadata.DatasetMetadata(\n",
        "\n",
        "    # use convenience function to build a Schema protobuf\n",
        "    schema_utils.schema_from_feature_spec({\n",
        "\n",
        "        # define a dictionary mapping the keys to its feature spec type\n",
        "        'y': tf.io.FixedLenFeature([], tf.float32),\n",
        "        'x': tf.io.FixedLenFeature([], tf.float32),\n",
        "        's': tf.io.FixedLenFeature([], tf.string),\n",
        "    }))"
      ],
      "metadata": {
        "id": "RPc5vxYMltKp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preview the schema\n",
        "print(raw_data_metadata._schema)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-1-sP4anvI4",
        "outputId": "e3fb8de2-e804-4563-ca11-efb75c79a6bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature {\n",
            "  name: \"s\"\n",
            "  type: BYTES\n",
            "  presence {\n",
            "    min_fraction: 1.0\n",
            "  }\n",
            "  shape {\n",
            "  }\n",
            "}\n",
            "feature {\n",
            "  name: \"x\"\n",
            "  type: FLOAT\n",
            "  presence {\n",
            "    min_fraction: 1.0\n",
            "  }\n",
            "  shape {\n",
            "  }\n",
            "}\n",
            "feature {\n",
            "  name: \"y\"\n",
            "  type: FLOAT\n",
            "  presence {\n",
            "    min_fraction: 1.0\n",
            "  }\n",
            "  shape {\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a preprocessing function\n",
        "The _preprocessing function_ is the most important concept of `tf.Transform`. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor'><code>Tensor</code></a> or <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor'><code>SparseTensor</code></a>. There are two main groups of API calls that typically form the heart of a preprocessing function:\n",
        "\n",
        "1. **TensorFlow Ops:** Any function that accepts and returns tensors. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time.  These will run for every example, during both training and serving.\n",
        "2. **TensorFlow Transform Analyzers:** Any of the analyzers provided by `tf.Transform`. Analyzers also accept and return tensors, but unlike TensorFlow ops they only run once during training, and typically make a full pass over the entire training dataset. They create <a target='_blank' href='https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/constant'>tensor constants</a>, which are added to your graph. For example, `tft.min` computes the minimum of a tensor over the training dataset.\n",
        "\n",
        "*Caution: When you apply your preprocessing function to serving inferences, the constants that were created by analyzers during training do not change.  If your data has trend or seasonality components, plan accordingly.*\n",
        "\n",
        "You can see available functions to transform your data [here](https://www.tensorflow.org/tfx/transform/api_docs/python/tft)."
      ],
      "metadata": {
        "id": "LYDT0DWuoGmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"Preprocess input columns into transformed columns.\"\"\"\n",
        "\n",
        "    # extract the columns and assign to local variables\n",
        "    x = inputs['x']\n",
        "    y = inputs['y']\n",
        "    s = inputs['s']\n",
        "\n",
        "    # data transformations using tft functions\n",
        "    x_centered = x - tft.mean(x)\n",
        "    y_normalized = tft.scale_to_0_1(y)\n",
        "    s_integerized = tft.compute_and_apply_vocabulary(s)\n",
        "    x_centered_times_y_normalized = (x_centered * y_normalized)\n",
        "\n",
        "    # return the transformed data\n",
        "    return {\n",
        "        'x_centered': x_centered,\n",
        "        'y_normalized': y_normalized,\n",
        "        's_integerized': s_integerized,\n",
        "        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n",
        "    }"
      ],
      "metadata": {
        "id": "fCNKnmExnxEc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate a constant graph with the required transformations"
      ],
      "metadata": {
        "id": "GsK2-hLPoyeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore the warnings\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# a temporary directory is needed when analyzing the data\n",
        "with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
        "\n",
        "    # define the pipeline using Apache Beam syntax\n",
        "    transformed_dataset, transform_fn = (\n",
        "\n",
        "        # analyze and transform the dataset using the preprocessing function\n",
        "        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n",
        "            preprocessing_fn)\n",
        "    )\n",
        "\n",
        "# unpack the transformed dataset\n",
        "transformed_data, transformed_metadata = transformed_dataset\n",
        "\n",
        "# print the results\n",
        "print('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\n",
        "print('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "BpjVfBA6oqUD",
        "outputId": "a786d80d-fcfd-495e-c1d7-a0a3cc8118aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
              "          var jqueryScript = document.createElement('script');\n",
              "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
              "          jqueryScript.type = 'text/javascript';\n",
              "          jqueryScript.onload = function() {\n",
              "            var datatableScript = document.createElement('script');\n",
              "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
              "            datatableScript.type = 'text/javascript';\n",
              "            datatableScript.onload = function() {\n",
              "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
              "              window.interactive_beam_jquery(document).ready(function($){\n",
              "                \n",
              "              });\n",
              "            }\n",
              "            document.head.appendChild(datatableScript);\n",
              "          };\n",
              "          document.head.appendChild(jqueryScript);\n",
              "        } else {\n",
              "          window.interactive_beam_jquery(document).ready(function($){\n",
              "            \n",
              "          });\n",
              "        }"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
            "WARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\n",
            "WARNING:absl:You are outputting instance dicts from `TransformDataset` which will not provide optimal performance. Consider setting  `output_record_batches=True` to upgrade to the TFXIO format (Apache Arrow RecordBatch). Encoding functionality in this module works with both formats.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-a1d6d3af-f073-4a8e-826a-f08c87014dac.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Raw data:\n",
            "[{'s': 'hello', 'x': 1, 'y': 1},\n",
            " {'s': 'world', 'x': 2, 'y': 2},\n",
            " {'s': 'hello', 'x': 3, 'y': 3}]\n",
            "\n",
            "Transformed data:\n",
            "[{'s_integerized': 0,\n",
            "  'x_centered': -1.0,\n",
            "  'x_centered_times_y_normalized': -0.0,\n",
            "  'y_normalized': 0.0},\n",
            " {'s_integerized': 1,\n",
            "  'x_centered': 0.0,\n",
            "  'x_centered_times_y_normalized': 0.0,\n",
            "  'y_normalized': 0.5},\n",
            " {'s_integerized': 0,\n",
            "  'x_centered': 1.0,\n",
            "  'x_centered_times_y_normalized': 1.0,\n",
            "  'y_normalized': 1.0}]\n"
          ]
        }
      ]
    }
  ]
}